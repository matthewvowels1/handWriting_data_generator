{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "import cv2\n",
    "import svgwrite\n",
    "from scipy import interpolate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from cairosvg import svg2png\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling = 0\n",
    "timesteps = 30 # (if resampling = 1)\n",
    "\n",
    "# download the training and validation data from https://github.com/emreaksan/deepwriting\n",
    "folder = '/media/matthewvowels/Storage/data/HandWriting/'\n",
    "data_dict = dict(np.load(folder+'deepwriting_training.npz', allow_pickle = True))\n",
    "\n",
    "## out filename:\n",
    "if resampling:\n",
    "    filename = '/media/matthewvowels/Storage/data/HandWriting/char_sequences_INTERPOLATED_{}_training.pickle'.format(timesteps)\n",
    "else:\n",
    "    filename = '/media/matthewvowels/Storage/data/HandWriting/char_sequences_training.pickle'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo_normalization(sample, detrend_sample=False):\n",
    "    # modified from https://github.com/emreaksan/deepwriting/blob/master/dataset_hw.py\n",
    "    sample_copy = np.copy(sample[:,:3])\n",
    "    if normalization:\n",
    "        sample_copy = sample_copy*norm_std+norm_mean\n",
    "    sample_copy[:,2] = sample[:,2]\n",
    "    return sample_copy\n",
    "\n",
    "def stroke_to_png_sequence(stroke, img_size, resampling, timesteps):\n",
    "    # modified from https://github.com/emreaksan/deepwriting/blob/master/dataset_hw.py\n",
    "    x = 0\n",
    "    y = 0\n",
    "    abs_ = []\n",
    "    for i in range(0, len(stroke)):\n",
    "        x += stroke[i,0]\n",
    "        y += stroke[i,1]\n",
    "        abs_.append([x,y])\n",
    "\n",
    "    abs_ = np.asarray(abs_)\n",
    "\n",
    "    if resampling:\n",
    "        x = np.linspace(0, timesteps, abs_.shape[0])\n",
    "        \n",
    "        abs_x = abs_[:,0]\n",
    "        f_x = interpolate.interp1d(x, abs_x)\n",
    "        abs_y = abs_[:,1]\n",
    "        f_y = interpolate.interp1d(x, abs_y)\n",
    "\n",
    "        x_new = np.linspace(0, timesteps, timesteps)\n",
    "        abs_x_new = np.expand_dims(f_x(x_new), 1)   \n",
    "        abs_y_new = np.expand_dims(f_y(x_new), 1)\n",
    "        abs_ = np.concatenate((abs_x_new, abs_y_new), 1)\n",
    "\n",
    "    \n",
    "    seq_len = len(abs_)\n",
    "    img_sequence = np.zeros((seq_len, image_size, image_size)).astype('uint8')\n",
    "    img_sequence[:,:,:] = 255\n",
    "    \n",
    "    r_x = abs_[:,0].max() - abs_[:,0].min()\n",
    "    r_y = abs_[:,1].max() - abs_[:,1].min()\n",
    "\n",
    "    if r_x >= r_y:\n",
    "        abs_ = (abs_ - abs_[:,0].min(axis=0)) / r_x\n",
    "        abs_ = abs_ * (0.9 - 0.1) + 0.1\n",
    "        abs_[:,1] += abs(abs_[:,1].min())\n",
    "        \n",
    "        abs_ *= (image_size - 4)\n",
    "        abs_[:,1] = abs_[:,1] - abs_[:,1].mean() + (image_size-1)/2\n",
    "    else:\n",
    "        abs_ = (abs_ - abs_[:,1].min(axis=0)) / r_y\n",
    "        abs_ = abs_ * (0.9 - 0.1) + 0.1\n",
    "        abs_[:,0] += abs(abs_[:,0].min())\n",
    "\n",
    "        abs_ *= (image_size - 4)\n",
    "        abs_[:,0] = abs_[:,0] - abs_[:,0].mean() + (image_size-1)/2\n",
    "        \n",
    "\n",
    "    abs_ = abs_.astype('int')\n",
    "\n",
    "    for j in range(0, len(abs_)):\n",
    "        cum_pen = abs_[:j+1]\n",
    "\n",
    "        for idx in range(0,cum_pen.shape[0]):\n",
    "\n",
    "            if (idx == 0):\n",
    "                start_p = (abs_[0,0], abs_[0,1])\n",
    "                end_p = (abs_[1,0], abs_[1,1] )\n",
    "                cv2.line(img_sequence[j], start_p, end_p, 0, 2, 8)\n",
    "            else:\n",
    "                end_p = (cum_pen[idx,0], cum_pen[idx,1] )\n",
    "                cv2.line(img_sequence[j], start_p, end_p, 0, 2, 8)\n",
    "                start_p = end_p\n",
    "        \n",
    "    img_sequence = 255 - img_sequence\n",
    "\n",
    "    return img_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_labels\n",
      "eow_labels\n",
      "max\n",
      "char_labels\n",
      "sow_labels\n",
      "min\n",
      "mean\n",
      "strokes\n",
      "eoc_labels\n",
      "soc_labels\n",
      "alphabet\n",
      "subject_labels\n",
      "texts\n",
      "std\n",
      "preprocessing\n",
      "['' '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'a' 'b' 'c' 'd' 'e' 'f' 'g'\n",
      " 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y'\n",
      " 'z' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q'\n",
      " 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' \"'\" '.' ',' '-' '(' ')' '/']\n",
      "(34577,)\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['', \"'\", '(', ')', ',', '-'], dtype='<U1')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in data_dict.keys():\n",
    "    print(key)\n",
    "print(data_dict['alphabet'])\n",
    "\n",
    "samples = data_dict['samples'] if 'samples' in data_dict.keys() else data_dict['strokes']\n",
    "print(samples.shape)\n",
    "char_labels = data_dict['char_labels']\n",
    "alphabet = data_dict['alphabet']\n",
    "eoc_labels = data_dict['eoc_labels']\n",
    "normalization = 'normalization' in data_dict['preprocessing']\n",
    "print(normalization)\n",
    "norm_mean = data_dict['mean']\n",
    "norm_std = data_dict['std']\n",
    "text = data_dict['texts']\n",
    "char_encoder = LabelEncoder()\n",
    "char_encoder.fit(alphabet)\n",
    "char_encoder.inverse_transform([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84474\n",
      "84474\n"
     ]
    }
   ],
   "source": [
    "unnormed_samples = []\n",
    "\n",
    "### Pull out sequences which are completed with ONE stroke ###\n",
    "\n",
    "for sample in samples:\n",
    "    unnormed_sample = undo_normalization(sample)\n",
    "    unnormed_samples.append(unnormed_sample)\n",
    "    \n",
    "unnormed_samples = np.asarray(unnormed_samples)\n",
    "\n",
    "sentence_n = 0\n",
    "single_strokes = []\n",
    "single_stroke_labels = []\n",
    "\n",
    "for sentence in eoc_labels:\n",
    "    seg_array = np.insert(np.argwhere(sentence == 1),[0],[0],axis=0)\n",
    "    \n",
    "    for i in range(len(seg_array)):\n",
    "        if (i+1) < len(seg_array):\n",
    "            character = unnormed_samples[sentence_n][int(seg_array[i]):int(seg_array[i+1])] \n",
    "            char_lab = char_labels[sentence_n][int(seg_array[i]+1):int(seg_array[i+1])] \n",
    "            if 1 not in character:\n",
    "                if len(char_lab) > 0:\n",
    "                    single_strokes.append(character)\n",
    "                    single_stroke_labels.append(char_lab[0])\n",
    "    sentence_n += 1\n",
    "single_stroke_labels = np.asarray(single_stroke_labels)\n",
    "print(len(single_stroke_labels))\n",
    "print(len(single_strokes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individually normalize each stroke\n",
    "eps = 10e-8\n",
    "for i in range(len(single_strokes)):\n",
    "    p = single_strokes[i][:,0].max()\n",
    "    q = single_strokes[i][:,1].max()\n",
    "    \n",
    "    if p > q:\n",
    "            \n",
    "        single_strokes[i][:,0] = single_strokes[i][:,0]  / (eps+p)               \n",
    "        single_strokes[i][:,1] = single_strokes[i][:,1] / (eps+p) \n",
    "    elif q > p:\n",
    "                                  \n",
    "        single_strokes[i][:,0] = single_strokes[i][:,0]  / (eps+q)               \n",
    "        single_strokes[i][:,1] = single_strokes[i][:,1] / (eps+q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the punctuation strokes \n",
    "inds = np.argsort(single_stroke_labels)\n",
    "single_stroke_labels = single_stroke_labels[inds]\n",
    "single_strokes = np.asarray(single_strokes)[inds]\n",
    "single_stroke_labels = single_stroke_labels[12:]\n",
    "single_strokes = single_strokes[12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_0\n",
      "M_1000\n",
      "W_2000\n",
      "a_3000\n",
      "a_4000\n",
      "a_5000\n",
      "a_6000\n",
      "b_7000\n",
      "c_8000\n",
      "d_9000\n",
      "d_10000\n",
      "e_11000\n",
      "e_12000\n",
      "e_13000\n",
      "e_14000\n",
      "e_15000\n",
      "e_16000\n",
      "e_17000\n",
      "e_18000\n",
      "e_19000\n",
      "e_20000\n",
      "e_21000\n",
      "e_22000\n",
      "e_23000\n",
      "e_24000\n",
      "e_25000\n",
      "e_26000\n",
      "e_27000\n",
      "e_28000\n",
      "f_29000\n",
      "g_30000\n",
      "h_31000\n",
      "h_32000\n",
      "h_33000\n",
      "h_34000\n",
      "h_35000\n",
      "i_36000\n",
      "i_37000\n",
      "i_38000\n",
      "i_39000\n",
      "i_40000\n",
      "i_41000\n",
      "i_42000\n",
      "k_43000\n",
      "l_44000\n",
      "l_45000\n",
      "l_46000\n",
      "m_47000\n",
      "m_48000\n",
      "n_49000\n",
      "n_50000\n",
      "n_51000\n",
      "n_52000\n",
      "n_53000\n",
      "n_54000\n",
      "n_55000\n",
      "o_56000\n",
      "o_57000\n",
      "o_58000\n",
      "o_59000\n",
      "o_60000\n",
      "o_61000\n",
      "p_62000\n",
      "r_63000\n",
      "r_64000\n",
      "r_65000\n",
      "r_66000\n",
      "r_67000\n",
      "r_68000\n",
      "r_69000\n",
      "r_70000\n",
      "s_71000\n",
      "s_72000\n",
      "s_73000\n",
      "s_74000\n",
      "t_75000\n",
      "t_76000\n",
      "t_77000\n",
      "t_78000\n",
      "u_79000\n",
      "u_80000\n",
      "v_81000\n",
      "w_82000\n",
      "y_83000\n",
      "y_84000\n"
     ]
    }
   ],
   "source": [
    "image_size = 64\n",
    "dict_ = {}\n",
    "\n",
    "for i in range(len(single_stroke_labels)):\n",
    "    \n",
    "    label = str(char_encoder.inverse_transform([single_stroke_labels[i]])[0]) \n",
    "    stroke = single_strokes[i]  \n",
    "    seq = stroke_to_png_sequence(stroke, image_size,resampling, timesteps) \n",
    "    if label in dict_.keys():\n",
    "        dict_[label] = dict_[label] + [seq]\n",
    "    else:\n",
    "        dict_[label] = [seq]\n",
    "\n",
    "    if i %1000 == 0:\n",
    "        print(label+'_' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving into  /media/matthewvowels/Storage/data/HandWriting/char_sequences_training.pickle\n"
     ]
    }
   ],
   "source": [
    "print('saving into ', filename)\n",
    "with open(filename, 'wb') as handle:\n",
    "    pickle.dump(dict_, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(filename, 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
